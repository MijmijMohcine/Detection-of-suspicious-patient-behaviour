{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Import the necessary modules.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from PyEmotion import *\n",
    "import cv2 as cv\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Load the pre-trained YOLOv5 model*\n",
    "\n",
    "*Load the fall detection model*\n",
    "\n",
    "*Create an PoseLandmarker object*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained YOLOv5 model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "\n",
    "# Set the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "fall_model = joblib.load(\"./detect_fall.joblib\")\n",
    "base_options_pos = python.BaseOptions(model_asset_path='pose_landmarker_lite.task')\n",
    "options_pos = vision.PoseLandmarkerOptions(\n",
    "    base_options=base_options_pos,\n",
    "    output_segmentation_masks=True)\n",
    "detector_pos = vision.PoseLandmarker.create_from_options(options_pos)\n",
    "er = DetectFace(device='cpu')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*detect human body*\n",
    "\n",
    "*draw  landmarks on image*\n",
    "\n",
    "*detect emotions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_position_on_image(rgb_image ,detector,model):\n",
    "    image = mp.Image(image_format=mp.ImageFormat.SRGB, data=np.array(rgb_image))\n",
    "    detection_result = detector.detect(image)\n",
    "    try:\n",
    "        body_positions = np.array(detection_result.pose_landmarks[0])\n",
    "\n",
    "        # Append the image filename and body positions to the data list\n",
    "        if body_positions is not None:\n",
    "            row = {}\n",
    "            for i in range(len(body_positions)):\n",
    "                row[\"x\"+str(i)] = body_positions[i].x\n",
    "                row[\"y\"+str(i)] = body_positions[i].y\n",
    "                row[\"z\"+str(i)] = body_positions[i].z\n",
    "            data = [row]\n",
    "        df2 = pd.DataFrame(data)\n",
    "        prediction = model.predict(df2)\n",
    "        return prediction[0], draw_landmarks_on_image(rgb_image, detection_result)\n",
    "    except:\n",
    "        -1, draw_landmarks_on_image(rgb_image, detection_result)\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "  pose_landmarks_list = detection_result.pose_landmarks\n",
    "  annotated_image = np.copy(rgb_image)\n",
    "\n",
    "  # Loop through the detected poses to visualize.\n",
    "  for idx in range(len(pose_landmarks_list)):\n",
    "    pose_landmarks = pose_landmarks_list[idx]\n",
    "\n",
    "    # Draw the pose landmarks.\n",
    "    pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "    pose_landmarks_proto.landmark.extend([\n",
    "      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in pose_landmarks\n",
    "    ])\n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "      annotated_image,\n",
    "      pose_landmarks_proto,\n",
    "      solutions.pose.POSE_CONNECTIONS,\n",
    "      solutions.drawing_styles.get_default_pose_landmarks_style())\n",
    "  return annotated_image\n",
    "\n",
    "\n",
    "\n",
    "def draw(frame):\n",
    "    pre = {-1:\"unkown\", 1: \"standing\", 0: \"falling\", 2:\"sitting\"}\n",
    "    TEXT_COLOR = (255, 0, 0)\n",
    "    MARGIN = 10\n",
    "    MARGIN = 10  # pixels\n",
    "    ROW_SIZE = 10  # pixels\n",
    "    FONT_SIZE = 1\n",
    "    FONT_THICKNESS = 1\n",
    "    results = model(frame)\n",
    "\n",
    "    # Retrieve the detected objects\n",
    "    detected_objects = results.xyxy[0]  # Index 0 represents the first detected object\n",
    "    \n",
    "    # Iterate over the detected objects and filter out human bodies\n",
    "    for detection in detected_objects:\n",
    "        class_label = detection[-1]\n",
    "        if class_label == 0:  # 0 represents the class label for humans\n",
    "            x1, y1, x2, y2, confidence, _ = detection\n",
    "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "            cv.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            frame_person = frame[y1:y2, x1:x2]\n",
    "            try:\n",
    "                frame_person, emotion = er.predict_emotion(frame_person)\n",
    "                pred, frame_person = extract_position_on_image(frame_person, detector_pos,fall_model)\n",
    "            except:\n",
    "                pred = -1\n",
    "            frame[y1:y2, x1:x2] = frame_person\n",
    "            text_location = (MARGIN + x1,\n",
    "                     MARGIN + ROW_SIZE + y1)\n",
    "            cv.putText(frame, str(pre[pred]), text_location, cv.FONT_HERSHEY_PLAIN, FONT_SIZE, TEXT_COLOR, FONT_THICKNESS)\n",
    "    return frame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**select a file from file explorer and apply the draw function**\n",
    "\n",
    "supported files: 'jpg', 'png', 'bmp', 'webp', 'mp4', 'avi', 'mov'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "\n",
    "# Create a Tkinter window\n",
    "window = tk.Tk()\n",
    "window.geometry(\"800x40\")\n",
    "file_path = \"\"\n",
    "# Function to open the file dialog and get the selected file path\n",
    "def select_file():\n",
    "    global file_path\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    entry.delete(0, tk.END)\n",
    "    entry.insert(tk.END, file_path)\n",
    "\n",
    "# Create a text entry widget to display the selected file path\n",
    "entry = tk.Entry(window,width=800)\n",
    "entry.pack()\n",
    "\n",
    "# Create a button to trigger the file dialog\n",
    "button = tk.Button(window, text=\"Select File\", command=select_file)\n",
    "button.pack()\n",
    "\n",
    "window.mainloop()\n",
    "\n",
    "if file_path:\n",
    "    print(\"Selected file:\", file_path)\n",
    "    _, file_extension = os.path.splitext(file_path)\n",
    "    output_path = os.path.splitext(file_path)[0]+'_output'+ file_extension\n",
    "    if file_extension.lower() in ['.jpg', '.png', '.bmp', '.webp']:\n",
    "        image = draw(cv.imread(file_path))\n",
    "        cv.imwrite(output_path,image)\n",
    "        print(f\"your file is ready in {output_path}\")\n",
    "        cv.imshow(\"output\", image)\n",
    "        cv.waitKey(0)\n",
    "    elif file_extension.lower() in ['.mp4', '.avi', '.mov']:\n",
    "        # Read in the video file\n",
    "        video = cv.VideoCapture(file_path)\n",
    "\n",
    "        # Get the frames per second (fps) of the video\n",
    "        fps = int(video.get(cv.CAP_PROP_FPS))\n",
    "\n",
    "        width = int(video.get(cv.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(video.get(cv.CAP_PROP_FRAME_HEIGHT))\n",
    "        total_frames = int(video.get(cv.CAP_PROP_FRAME_COUNT))\n",
    "        writer = cv.VideoWriter(output_path, int(video.get(cv.CAP_PROP_FOURCC)), fps, (width, height))\n",
    "\n",
    "        # Loop through the video frames\n",
    "        with tqdm(total=total_frames, desc='Processing frames') as pbar:\n",
    "            frame_count = 0\n",
    "            while True:\n",
    "                # Read in a single frame\n",
    "                ret, frame = video.read()\n",
    "                \n",
    "                # Check if we have reached the end of the video\n",
    "                if not ret:\n",
    "                    break\n",
    "                \n",
    "                # Write the edited frame to the output file\n",
    "                writer.write(draw(frame))\n",
    "                pbar.update(1)\n",
    "                frame_count += 1\n",
    "                pbar.set_postfix({'Frame': frame_count})\n",
    "            \n",
    "        # Release the resources used by the VideoCapture and VideoWriter objects\n",
    "        video.release()\n",
    "        writer.release()\n",
    "        print(f\"your file is ready in {output_path}\")\n",
    "    else:\n",
    "        print('unkown input')\n",
    "else:\n",
    "    print(\"No file selected.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Live stream using OpenCV*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the webcam video stream\n",
    "cap = cv.VideoCapture(0)\n",
    "\n",
    "\n",
    "# Create a named window to display the live stream\n",
    "cv.namedWindow(\"Live Stream\", cv.WINDOW_NORMAL)\n",
    "\n",
    "# Loop through the video frames\n",
    "while True:\n",
    "    # Read a frame from the video stream\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Process the frame\n",
    "    frame = draw(frame)\n",
    "\n",
    "    # Break the loop on 'q' key press\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video stream and close any open windows\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
